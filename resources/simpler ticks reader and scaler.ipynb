{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simpler Scaler and Encoder\n",
    "\n",
    "The idea with this notebook is to achieve a simpler read/scale/encode/decode/unscale process for `trader`.\n",
    "\n",
    "It must be based on simple contracts between objects, which are also based on standard classes (DataFrames, ndarrays) instead of specific objects from the project.\n",
    "\n",
    "Expected usage:\n",
    "\n",
    "    # Read the OHLCV file\n",
    "    ticks = Ticks(url, scale=True, *args, **kwargs)\n",
    "    \n",
    "    # Original data read from the file can be accessed easily\n",
    "    ticks.raw\n",
    "    \n",
    "    # Scaled data is also accessible\n",
    "    ticks.data\n",
    "    \n",
    "    # If we need to encode it, we can also do it, and later access \n",
    "    # the result from within the object property,\n",
    "    ticks.encode()\n",
    "    ticks.encoded\n",
    "    \n",
    "    # ... or simply capture the output from the encode() method\n",
    "    encoded = ticks.encode()\n",
    "    \n",
    "One we have the ticks read from the file, we might need to prepare them for the LSTM. To do that we must use another specific class, called Sequences.\n",
    "\n",
    "The contract with the class is either a multivariate time series, stored in a dataframe. So we can pass an:\n",
    "\n",
    "  - Univariate time series with float values\n",
    "  - Univariate time series with binary values (encoded)\n",
    "  - Multivariate time series with floats\n",
    "  - Multivariate time series with binary values (encoded)\n",
    " \n",
    "Sequences needs to know what is the total number of time series in the data (the number of variables or categories) and the number of time steps in which the time series will be grouped to be later feed into the recurrent networks. An optional final parameter also needed is the test size ratio (0.1, 0.2, etc.) which takes a value of 0.1 by default.\n",
    "\n",
    "The expected use is as follows:\n",
    "\n",
    "    # If we want to prepare the data in Ticks object, unencoded:\n",
    "    X_train, y_train, X_test, y_test = sequences.prepare(\n",
    "        ticks.data, \n",
    "        timesteps=window_size, \n",
    "        test_size=test_size\n",
    "    )\n",
    "    \n",
    "or\n",
    "\n",
    "    # If we want to prepare the data in Ticks object ENCODED:\n",
    "    ticks.encode()\n",
    "    X_train, y_train, X_test, y_test = sequences.prepare(\n",
    "        ticks.encoded,\n",
    "        timesteps=window_size\n",
    "    )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import ndarray\n",
    "from sklearn.preprocessing import RobustScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pandas import DataFrame, Series\n",
    "from pathlib import Path\n",
    "from math import ceil, floor\n",
    "from numpy import ndarray\n",
    "from typing import Tuple, Union\n",
    "\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black\n",
    "\n",
    "url = \"/Users/renero/Documents/SideProjects/trader/data/^GDAXI/DAX_2018_2019.csv\"\n",
    "scaler_filename = \"/Users/renero/robust_scaler.gz\"\n",
    "\n",
    "\n",
    "class my_dict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "pd.set_option(\"max_seq_items\", 1000)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ticks Class definition (âš¡ï¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ticks:\n",
    "    \"\"\"Ticks data read from file with all the relevant parameters and metadata from it\"\"\"\n",
    "\n",
    "    data = None\n",
    "    _scaler = None\n",
    "    scaler_file = None\n",
    "    _encoder = None\n",
    "    _volatility = 0.0\n",
    "\n",
    "    params = my_dict({\"ohlc\": [\"open\", \"high\", \"low\", \"close\"]})\n",
    "\n",
    "    def __init__(self, url, scale=False):\n",
    "        data = pd.read_csv(url).round(2)\n",
    "\n",
    "        format = \"%Y-%m-%d %H:%M:%S\"\n",
    "        data[\"Datetime\"] = pd.to_datetime(data[\"Date\"] + \" 00:00:00\", format=format)\n",
    "        data = data.set_index(pd.DatetimeIndex(data[\"Datetime\"]))\n",
    "        data = data.drop([\"Date\", \"Datetime\", \"Volume\"], axis=1)\n",
    "        data.columns = self.params.ohlc\n",
    "        self.data = data\n",
    "        self.raw = self.data.copy(deep=True)\n",
    "        if scale is True:\n",
    "            self.data = self.transform()\n",
    "        self._volatility = self.data.close.std()\n",
    "\n",
    "    def transform(self, inline: bool = False) -> DataFrame:\n",
    "        \"\"\"Scales the OHLC ticks from the dataframe read\"\"\"\n",
    "        self._scaler = RobustScaler().fit(self.data[self.params.ohlc])\n",
    "        scaled_df = pd.DataFrame(\n",
    "            data=self._scaler.transform(self.data[self.params.ohlc]),\n",
    "            columns=self.params.ohlc,\n",
    "            index=self.data.index,\n",
    "        ).round(2)\n",
    "        if not inline:\n",
    "            return scaled_df\n",
    "        self.raw = self.data.copy(deep=True)\n",
    "        self.data = scaled_df.copy(deep=True)\n",
    "        self._volatility = self.data.close.std()\n",
    "        return self.data\n",
    "\n",
    "    def inverse_transform(self, scaled_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Scales back a dataframe with the scaler stored in filename\"\"\"\n",
    "        assert self._scaler is not None, \"Scaler must be set first (scale())\"\n",
    "        return pd.DataFrame(\n",
    "            data=self._scaler.inverse_transform(scaled_df[self.params.ohlc]),\n",
    "            columns=self.params.ohlc,\n",
    "            index=scaled_df.index,\n",
    "        ).round(2)\n",
    "\n",
    "    def encode(self, num_slots=10) -> np.ndarray:\n",
    "        \"\"\"Encode (OneHot) the discretized version of values in attribute `data`\"\"\"\n",
    "        assert (\n",
    "            self._scaler is not None\n",
    "        ), \"Data has not been scaled yet. Use 'transform()'\"\n",
    "        self._discretized = self._discretize(num_slots)\n",
    "        self._edge = max(\n",
    "            self._discretized.max().max(), abs(self._discretized.min().min())\n",
    "        )\n",
    "        self._categories = [\n",
    "            np.arange(-self._edge, self._edge + 1)\n",
    "            for _ in range(len(self.data.columns))\n",
    "        ]\n",
    "        self._encoder = OneHotEncoder(\n",
    "            handle_unknown=\"ignore\", categories=self._categories\n",
    "        ).fit(self._discretized)\n",
    "        self._encoded = self._encoder.transform(self._discretized).toarray()\n",
    "        return self._encoded\n",
    "\n",
    "    def decode(self, prediction: int, prev_value: float, verbose=False):\n",
    "        activated = np.argmax(prediction)\n",
    "        if verbose:\n",
    "            print(f\"Cell activated: {activated} out of {prediction.shape[0]}\")\n",
    "            # print(f\"Value: {np.round(prediction[activated],2)}\")\n",
    "\n",
    "        pred_binary = np.identity(prediction.shape[0], int)[prediction.argmax()]\n",
    "        if verbose:\n",
    "            print(f\"Prediction hardmaxed: {pred_binary}\")\n",
    "            print(f\"Categories used in encoder: {ticks._categories[0]}\")\n",
    "\n",
    "        # Have to copy the response from the net 4 times, as the encoder is 'fitted'\n",
    "        # with 4 vectors, corresponding to O/H/L/C.\n",
    "        to_decode = np.tile(pred_binary, ticks.data.shape[1]).reshape(\n",
    "            1, X_train.shape[2]\n",
    "        )\n",
    "        decoded_bin = ticks._encoder.inverse_transform(to_decode)[0][0]\n",
    "        diff_with_previous = (\n",
    "            np.sign(decoded_bin) * np.abs(ticks._bins[decoded_bin])\n",
    "        ) - (self._bin_size / 2.0)\n",
    "        decoded_value = np.round(\n",
    "            (prev_value + (diff_with_previous * self._volatility)), 2\n",
    "        )\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Decoded_diff: {decoded_bin}, corresponding to {diff_with_previous}% change\"\n",
    "            )\n",
    "            print(f\">> DECODED value: {decoded_value} (previous: {prev_value})\")\n",
    "\n",
    "        return decoded_value\n",
    "\n",
    "    def _discretize(self, num_slots=10) -> DataFrame:\n",
    "        \"\"\"Discretize the rolling difference in num_slots of volatility of time series\"\"\"\n",
    "        self._num_slots = num_slots\n",
    "        self._diffed = self._diff()\n",
    "        self._bin_size = ticks._volatility / self._num_slots\n",
    "        self._bins = np.arange(0.0, self._volatility + self._bin_size, self._bin_size)[\n",
    "            1:\n",
    "        ]\n",
    "        self._bins = np.around(self._bins, decimals=2)\n",
    "        return self._diffed.apply(\n",
    "            lambda row: round(row / self._bin_size), result_type=\"expand\", axis=1\n",
    "        ).astype(int)\n",
    "\n",
    "    def _diff(self) -> DataFrame:\n",
    "        \"\"\"Returns a dataframe with each row being the difference with its previous one\"\"\"\n",
    "        return (\n",
    "            self.data.rolling(window=2)\n",
    "            .apply(lambda row: row.iloc[1] - row.iloc[0])\n",
    "            .round(2)\n",
    "            .fillna(0.00)\n",
    "        )\n",
    "\n",
    "    def save_scaler(self, filename: str) -> None:\n",
    "        \"\"\"\n",
    "        Saves the scaler with the filename specified\n",
    "        \n",
    "        :param filename: The complete path and filename where the scaler will be saved\n",
    "        :returns: None\n",
    "        :raises AssertionError: raises if scaler is not set before trying to save it\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            self._scaler is not None\n",
    "        ), \"Scaler has not yet been created. Use scale() method first.\"\n",
    "        self.scaler_file = filename\n",
    "        joblib.dump(self._scaler, self._scaler_file)\n",
    "        print(f\"RobustScaler saved at: {self._scaler_file}\")\n",
    "\n",
    "    def load_scaler(self, filename: str):\n",
    "        \"\"\"Loads the scaler from the filename specified\"\"\"\n",
    "        self.scaler_file = filename\n",
    "        self._scaler = joblib.load(self._scaler_file)\n",
    "        return self._scaler\n",
    "\n",
    "    @property\n",
    "    def scaler_file(self) -> str:\n",
    "        if hasattr(self, \"_scaler_file\"):\n",
    "            return self._scaler_file.as_posix()\n",
    "        return None\n",
    "\n",
    "    @scaler_file.setter\n",
    "    def scaler_file(self, filename):\n",
    "        file = Path(filename)\n",
    "        if not file.is_file():\n",
    "            raise ValueError(\"Filename for scaler does not exist\")\n",
    "        self._scaler_file = file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1\n",
    "Read CSV, scale and back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            open  high   low  close\n",
      "Datetime                           \n",
      "2018-01-02  0.71  0.66  0.61   0.67\n",
      "2018-01-03  0.73  0.78  0.77   0.79\n",
      "2018-01-04  0.90  0.99  0.96   1.00\n",
      "                open      high       low     close\n",
      "Datetime                                          \n",
      "2018-01-02  12896.00  12921.95  12744.11  12867.73\n",
      "2018-01-03  12914.15  13027.61  12889.98  12976.55\n",
      "2018-01-04  13068.50  13212.51  13063.20  13166.97\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14fcdbb90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAECCAYAAAD3pb/YAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOIUlEQVR4nO3dX4zl5VkH8O8jyx/FkrKFEgpoq6KGi3ZNJlAjFxRsS2sjNTGmRBMumuCFTWqiMehN1aSJXmj1ojFBS+DCUhsUSwyRAiXBC1O7a7GlthUkNGVLWQltxJhQoI8Xc0jH6SyzM+ecOee37+eTbOb83jmZ35M877775eU3563uDgAAjOAHVl0AAAAcFOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYxkrDb1VdX1VfrarHq+qWVdbC7qrqtqo6UVWPbhk7XFX3V9Vjs6/nr7JGdlZVl1XVQ1X171X1par64Gxc/yagqs6pqn+pqn+b9e8PZuNvqqrPztbQv6mqs1ZdKzurqjOq6vNV9Q+za72biKp6sqq+WFWPVNXR2Zi1c8JWFn6r6owkH03yriRXJLmxqq5YVT2cktuTXL9t7JYkD3b35UkenF2zfl5K8lvdfUWStyb5jdnfN/2bhheSXNvdb0lyJMn1VfXWJH+c5CPd/RNJvpXk/asrkV18MMmXt1zr3bS8rbuPdPfG7NraOWGr3Pm9Msnj3f1Ed38nySeS3LDCethFdz+c5LltwzckuWP2+o4k7z3Imjg13f10d//r7PXz2fxH+JLo3yT0pv+ZXZ45+9NJrk1y12xc/9ZUVV2a5BeS/NXsuqJ3U2ftnLBVht9Lknx9y/VTszGm5aLufnr2+ptJLlplMeyuqt6Y5GeSfDb6Nxmz/23+SJITSe5P8p9Jvt3dL83eYg1dX3+W5HeSfHd2/bro3ZR0kk9X1bGqunk2Zu2csEOrLoDTR3d3VTkve41V1Q8n+dskv9nd/725AbVJ/9Zbd7+c5EhVvTbJ3Ul+erUVcSqq6j1JTnT3saq6ZsXlsD9Xd/fxqnp9kvur6itbv2ntnJ5V7vweT3LZlutLZ2NMyzNVdXGSzL6eWHE9nERVnZnN4PvX3f13s2H9m5ju/naSh5L8bJLXVtUrmxjW0PX0c0l+saqezObjfdcm+fPo3WR09/HZ1xPZ/A/PK2PtnLRVht/PJbl89huvZyV5X5J7VlgP+3NPkptmr29K8qkV1sJJzJ4x/FiSL3f3n275lv5NQFVdONvxTVX9YJK3Z/O57YeS/PLsbfq3hrr7d7v70u5+Yzb/nftMd/9q9G4SqurcqnrNK6+TvCPJo7F2Tlp1r26nvqrenc1noc5Iclt3f3hlxbCrqrozyTVJLkjyTJIPJfn7JJ9M8iNJvpbkV7p7+y/FsWJVdXWSf0ryxXzvucPfy+Zzv/q35qrqzdn8pZozsrlp8cnu/sOq+rFs7iYeTvL5JL/W3S+srlJezeyxh9/u7vfo3TTM+nT37PJQko9394er6nWxdk7WSsMvAAAcJCe8AQAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAw1iL8LvluEAmRu+mTf+mS++mTf+mTf+mbS3CbxKTaLr0btr0b7r0btr0b9r0b8LWJfwCAMDSHeghF2fV2X1Ozv2+8RfzQs7M2QdWx9T85Jv/95Tf+x9f+KElVvL99G7aFtG/dZ6fpzN/93a3znNT/6bN2jkNz+dbz3b3hdvHDzT8nleH+6q67sDud7q47xuPnPJ73/mGI0urA3ZifrKuzE3Wmfm5fA/0Xce6e2P7+FyPPVTV9VX11ap6vKpumednAQDAsu07/FbVGUk+muRdSa5IcmNVXbGowgAAYNHm2fm9Msnj3f1Ed38nySeS3LCYsgAAYPHmCb+XJPn6luunZmMAALCWDi37BrMPgr45Sc6J31YEAGB15tn5PZ7ksi3Xl87G/p/uvrW7N7p7w8e6AACwSvOE388lubyq3lRVZyV5X5J7FlMWAAAs3r4fe+jul6rqA0nuS3JGktu6+0sLqwwAABZsrmd+u/veJPcuqBZggpb14es+AJ55rcPcXGYdTNs6zM9R5+Zch1wAAMCUCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYQi/AAAMo7r7wG52Xh3uq+q6A7sfr84RnZwuHOfJOjM/WVen+9x8oO861t0b28ft/AIAMAzhFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADOPQqgsA2MnpfuY807WXuZmYnxwsa+fu7PwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDCEXwAAhiH8AgAwDOEXAIBhON54YHs91tCRiczLHALYO2vnYtn5BQBgGMIvAADDEH4BABiG8AsAwDCEXwAAhiH8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAw3C88QSsy7GGjkxkJ+syP2G7Zc1N85hFsHaujp1fAACGIfwCADCMuR57qKonkzyf5OUkL3X3xiKKAgCAZVjEM79v6+5nF/BzAABgqTz2AADAMOYNv53k01V1rKpu3ukNVXVzVR2tqqMv5oU5bwcAAPs372MPV3f38ap6fZL7q+or3f3w1jd0961Jbk2S8+pwz3k/AADYt7l2frv7+OzriSR3J7lyEUUBAMAy7Dv8VtW5VfWaV14neUeSRxdVGAAALNo8jz1clOTuqnrl53y8u/9xIVUBAMAS7Dv8dvcTSd6ywFqANbCXIzcTx24yP8cQs66WeQSx+bk6PuoMAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYQi/AAAMY9/HGzOfZR6ZCDAle1njrJ0cpGXNzb3+bBbLzi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDAOrboAADhV73zDkVN+733feGQpPxd2ssw5ZC4vlp1fAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDCEXwAAhiH8AgAwDMcbr4gjOllXe51D5ifA3tbCZG/robVzsez8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYTjeGJjLso7ddGwyMCXLXIesh4tl5xcAgGHsGn6r6raqOlFVj24ZO1xV91fVY7Ov5y+3TAAAmN+p7PzenuT6bWO3JHmwuy9P8uDsGgAA1tqu4be7H07y3LbhG5LcMXt9R5L3LrYsAABYvP0+83tRdz89e/3NJBctqB4AAFiauX/hrbs7SZ/s+1V1c1UdraqjL+aFeW8HAAD7tt/w+0xVXZwks68nTvbG7r61uze6e+PMnL3P2wEAwPz2G37vSXLT7PVNST61mHIAAGB5TuWjzu5M8s9Jfqqqnqqq9yf5oyRvr6rHkvz87BoAANbarie8dfeNJ/nWdQuuBQAAlsrxxjCAKR6NuS51sF6mOJcZw17mZrK3+WkuL5bjjQEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDCquw/sZufV4b6qrjuw+43I0Z+sM/OTdWVucrowl7/ngb7rWHdvbB+38wsAwDCEXwAAhiH8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMIxDqy4AGMdezpF3Pj0Ay2DnFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADKO6+8Budl4d7qvqugO7H69uL8fHJo6QZWeOIWY01k4OmnV2fx7ou45198b2cTu/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGIdWXQAwbcs6StNxngAsg51fAACGsWv4rarbqupEVT26Zez3q+p4VT0y+/Pu5ZYJAADzO5Wd39uTXL/D+Ee6+8jsz72LLQsAABZv1/Db3Q8nee4AagEAgKWa55nfD1TVF2aPRZy/sIoAAGBJ9ht+/yLJjyc5kuTpJH9ysjdW1c1VdbSqjr6YF/Z5OwAAmN++wm93P9PdL3f3d5P8ZZIrX+W9t3b3RndvnJmz91snAADMbV/ht6ou3nL5S0kePdl7AQBgXex6yEVV3ZnkmiQXVNVTST6U5JqqOpKkkzyZ5NeXVyIAACzGruG3u2/cYfhjS6gFAACWqrr7wG52Xh3uq+q6A7vf6cIxr6wz85N5mUOsK3Nz2h7ou45198b2cccbAwAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYRxadQHrbh2ONnRkIvNa5jw2P9nJOqydADux8wsAwDCEXwAAhiH8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMIxDqy5g3S3rzHnn3gOns6mtW9ZkdrIuvTY/F8vOLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbjjVfEsckcpGX22pxjXZmbrDPzc3Xs/AIAMAzhFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGFUdx/czar+K8nXdvjWBUmePbBCWCS9mzb9my69mzb9mzb9m4Yf7e4Ltw8eaPg9mao62t0bq66DvdO7adO/6dK7adO/adO/afPYAwAAwxB+AQAYxrqE31tXXQD7pnfTpn/TpXfTpn/Tpn8TthbP/AIAwEFYl51fAABYOuEXAIBhCL8AAAxD+AUAYBjCLwAAw/g/mzp3NpMibfwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ticks = Ticks(url, scale=True)\n",
    "print(ticks.data.head(3))\n",
    "\n",
    "reversed_df = ticks.inverse_transform(ticks.data)\n",
    "print(reversed_df.head(3))\n",
    "\n",
    "trf = ticks.encode()\n",
    "plt.matshow(trf[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14fde0d90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ticks = Ticks(url)\n",
    "ticks.transform(inline=True)\n",
    "matrix = ticks.encode()\n",
    "plt.matshow(matrix[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustScaler saved at: /Users/renero/robust_scaler.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x14febb750>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAr8AAAECCAYAAAD3pb/YAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOIUlEQVR4nO3dX4zl5VkH8O8jyx/FkrKFEgpoq6KGi3ZNJlAjFxRsS2sjNTGmRBMumuCFTWqiMehN1aSJXmj1ojFBS+DCUhsUSwyRAiXBC1O7a7GlthUkNGVLWQltxJhQoI8Xc0jH6SyzM+ecOee37+eTbOb83jmZ35M877775eU3563uDgAAjOAHVl0AAAAcFOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYxkrDb1VdX1VfrarHq+qWVdbC7qrqtqo6UVWPbhk7XFX3V9Vjs6/nr7JGdlZVl1XVQ1X171X1par64Gxc/yagqs6pqn+pqn+b9e8PZuNvqqrPztbQv6mqs1ZdKzurqjOq6vNV9Q+za72biKp6sqq+WFWPVNXR2Zi1c8JWFn6r6owkH03yriRXJLmxqq5YVT2cktuTXL9t7JYkD3b35UkenF2zfl5K8lvdfUWStyb5jdnfN/2bhheSXNvdb0lyJMn1VfXWJH+c5CPd/RNJvpXk/asrkV18MMmXt1zr3bS8rbuPdPfG7NraOWGr3Pm9Msnj3f1Ed38nySeS3LDCethFdz+c5LltwzckuWP2+o4k7z3Imjg13f10d//r7PXz2fxH+JLo3yT0pv+ZXZ45+9NJrk1y12xc/9ZUVV2a5BeS/NXsuqJ3U2ftnLBVht9Lknx9y/VTszGm5aLufnr2+ptJLlplMeyuqt6Y5GeSfDb6Nxmz/23+SJITSe5P8p9Jvt3dL83eYg1dX3+W5HeSfHd2/bro3ZR0kk9X1bGqunk2Zu2csEOrLoDTR3d3VTkve41V1Q8n+dskv9nd/725AbVJ/9Zbd7+c5EhVvTbJ3Ul+erUVcSqq6j1JTnT3saq6ZsXlsD9Xd/fxqnp9kvur6itbv2ntnJ5V7vweT3LZlutLZ2NMyzNVdXGSzL6eWHE9nERVnZnN4PvX3f13s2H9m5ju/naSh5L8bJLXVtUrmxjW0PX0c0l+saqezObjfdcm+fPo3WR09/HZ1xPZ/A/PK2PtnLRVht/PJbl89huvZyV5X5J7VlgP+3NPkptmr29K8qkV1sJJzJ4x/FiSL3f3n275lv5NQFVdONvxTVX9YJK3Z/O57YeS/PLsbfq3hrr7d7v70u5+Yzb/nftMd/9q9G4SqurcqnrNK6+TvCPJo7F2Tlp1r26nvqrenc1noc5Iclt3f3hlxbCrqrozyTVJLkjyTJIPJfn7JJ9M8iNJvpbkV7p7+y/FsWJVdXWSf0ryxXzvucPfy+Zzv/q35qrqzdn8pZozsrlp8cnu/sOq+rFs7iYeTvL5JL/W3S+srlJezeyxh9/u7vfo3TTM+nT37PJQko9394er6nWxdk7WSsMvAAAcJCe8AQAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAw1iL8LvluEAmRu+mTf+mS++mTf+mTf+mbS3CbxKTaLr0btr0b7r0btr0b9r0b8LWJfwCAMDSHeghF2fV2X1Ozv2+8RfzQs7M2QdWx9T85Jv/95Tf+x9f+KElVvL99G7aFtG/dZ6fpzN/93a3znNT/6bN2jkNz+dbz3b3hdvHDzT8nleH+6q67sDud7q47xuPnPJ73/mGI0urA3ZifrKuzE3Wmfm5fA/0Xce6e2P7+FyPPVTV9VX11ap6vKpumednAQDAsu07/FbVGUk+muRdSa5IcmNVXbGowgAAYNHm2fm9Msnj3f1Ed38nySeS3LCYsgAAYPHmCb+XJPn6luunZmMAALCWDi37BrMPgr45Sc6J31YEAGB15tn5PZ7ksi3Xl87G/p/uvrW7N7p7w8e6AACwSvOE388lubyq3lRVZyV5X5J7FlMWAAAs3r4fe+jul6rqA0nuS3JGktu6+0sLqwwAABZsrmd+u/veJPcuqBZggpb14es+AJ55rcPcXGYdTNs6zM9R5+Zch1wAAMCUCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYQi/AAAMo7r7wG52Xh3uq+q6A7sfr84RnZwuHOfJOjM/WVen+9x8oO861t0b28ft/AIAMAzhFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADOPQqgsA2MnpfuY807WXuZmYnxwsa+fu7PwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDCEXwAAhiH8AgAwDOEXAIBhON54YHs91tCRiczLHALYO2vnYtn5BQBgGMIvAADDEH4BABiG8AsAwDCEXwAAhiH8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAw3C88QSsy7GGjkxkJ+syP2G7Zc1N85hFsHaujp1fAACGIfwCADCMuR57qKonkzyf5OUkL3X3xiKKAgCAZVjEM79v6+5nF/BzAABgqTz2AADAMOYNv53k01V1rKpu3ukNVXVzVR2tqqMv5oU5bwcAAPs372MPV3f38ap6fZL7q+or3f3w1jd0961Jbk2S8+pwz3k/AADYt7l2frv7+OzriSR3J7lyEUUBAMAy7Dv8VtW5VfWaV14neUeSRxdVGAAALNo8jz1clOTuqnrl53y8u/9xIVUBAMAS7Dv8dvcTSd6ywFqANbCXIzcTx24yP8cQs66WeQSx+bk6PuoMAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYQi/AAAMY9/HGzOfZR6ZCDAle1njrJ0cpGXNzb3+bBbLzi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDAOrboAADhV73zDkVN+733feGQpPxd2ssw5ZC4vlp1fAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDCEXwAAhiH8AgAwDMcbr4gjOllXe51D5ifA3tbCZG/robVzsez8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYTjeGJjLso7ddGwyMCXLXIesh4tl5xcAgGHsGn6r6raqOlFVj24ZO1xV91fVY7Ov5y+3TAAAmN+p7PzenuT6bWO3JHmwuy9P8uDsGgAA1tqu4be7H07y3LbhG5LcMXt9R5L3LrYsAABYvP0+83tRdz89e/3NJBctqB4AAFiauX/hrbs7SZ/s+1V1c1UdraqjL+aFeW8HAAD7tt/w+0xVXZwks68nTvbG7r61uze6e+PMnL3P2wEAwPz2G37vSXLT7PVNST61mHIAAGB5TuWjzu5M8s9Jfqqqnqqq9yf5oyRvr6rHkvz87BoAANbarie8dfeNJ/nWdQuuBQAAlsrxxjCAKR6NuS51sF6mOJcZw17mZrK3+WkuL5bjjQEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGMIvAADDEH4BABiG8AsAwDCquw/sZufV4b6qrjuw+43I0Z+sM/OTdWVucrowl7/ngb7rWHdvbB+38wsAwDCEXwAAhiH8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMIxDqy4AGMdezpF3Pj0Ay2DnFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADKO6+8Budl4d7qvqugO7H69uL8fHJo6QZWeOIWY01k4OmnV2fx7ou45198b2cTu/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGEIvwAADEP4BQBgGIdWXQAwbcs6StNxngAsg51fAACGsWv4rarbqupEVT26Zez3q+p4VT0y+/Pu5ZYJAADzO5Wd39uTXL/D+Ee6+8jsz72LLQsAABZv1/Db3Q8nee4AagEAgKWa55nfD1TVF2aPRZy/sIoAAGBJ9ht+/yLJjyc5kuTpJH9ysjdW1c1VdbSqjr6YF/Z5OwAAmN++wm93P9PdL3f3d5P8ZZIrX+W9t3b3RndvnJmz91snAADMbV/ht6ou3nL5S0kePdl7AQBgXex6yEVV3ZnkmiQXVNVTST6U5JqqOpKkkzyZ5NeXVyIAACzGruG3u2/cYfhjS6gFAACWqrr7wG52Xh3uq+q6A7vf6cIxr6wz85N5mUOsK3Nz2h7ou45198b2cccbAwAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYRxadQHrbh2ONnRkIvNa5jw2P9nJOqydADux8wsAwDCEXwAAhiH8AgAwDOEXAIBhCL8AAAxD+AUAYBjCLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMIxDqy5g3S3rzHnn3gOns6mtW9ZkdrIuvTY/F8vOLwAAwxB+AQAYhvALAMAwhF8AAIYh/AIAMAzhFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbjjVfEsckcpGX22pxjXZmbrDPzc3Xs/AIAMAzhFwCAYQi/AAAMQ/gFAGAYwi8AAMMQfgEAGIbwCwDAMIRfAACGIfwCADAM4RcAgGFUdx/czar+K8nXdvjWBUmePbBCWCS9mzb9my69mzb9mzb9m4Yf7e4Ltw8eaPg9mao62t0bq66DvdO7adO/6dK7adO/adO/afPYAwAAwxB+AQAYxrqE31tXXQD7pnfTpn/TpXfTpn/Tpn8TthbP/AIAwEFYl51fAABYOuEXAIBhCL8AAAxD+AUAYBjCLwAAw/g/mzp3NpMibfwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ticks = Ticks(url)\n",
    "ticks.transform(inline=True)\n",
    "ticks.save_scaler(scaler_filename)\n",
    "matrix = ticks.encode()\n",
    "plt.matshow(matrix[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Sequences (âš¡ï¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sequences:\n",
    "    \"\"\"\n",
    "    From a dataframe with four categories (OHLC) in a multivariate timeseries,\n",
    "    aggrupate it in timesteps windows, split it in training and testing subsets, and \n",
    "    finally, aggrupate X and y values together.\n",
    "\n",
    "    Consider a given multi-variate sequence (num_categories = 3):\n",
    "\n",
    "        [[ 10  15  25]\n",
    "         [ 20  25  45]\n",
    "         [ 30  35  65]\n",
    "         [ 40  45  85]\n",
    "         [ 50  55 105]\n",
    "         ...\n",
    "        ]\n",
    "\n",
    "    We can divide the sequence into multiple input/output patterns called samples, \n",
    "    where three time steps are used as input and one time step is used as output for \n",
    "    the one-step prediction that is being learned.\n",
    "\n",
    "         X,             y_3\n",
    "        --------------------\n",
    "        [10, 15, 25\t\n",
    "         20, 25, 45\t\n",
    "         30, 35, 65]    85\n",
    "        [20, 25, 45\n",
    "         30, 35, 65\n",
    "         40, 45, 85]    105\n",
    "        ...\n",
    "\n",
    "    In this example we consider timesteps=3, so X is composed of \n",
    "    [ timesteps x num_categories ] samples, and y is the third column of \n",
    "    the next sample.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def prepare(\n",
    "        cls, df: DataFrame, timesteps: int, test_size: float = 0.1\n",
    "    ) -> Union[Tuple[ndarray, ndarray, ndarray, ndarray], Tuple[ndarray, ndarray]]:\n",
    "        \"\"\"\n",
    "        Prepare the input dataframe (OHLC) converting it into an ndarray\n",
    "        of (num_samples x timesteps x num_categories), also splitting it\n",
    "        into training and test sets.\n",
    "        \"\"\"\n",
    "        num_categories = df.shape[1]\n",
    "\n",
    "        df = cls.aggrupate_in_timesteps(df, timesteps)\n",
    "        if test_size != 0.0:\n",
    "            train, test = train_test_split(df, test_size=test_size, shuffle=False)\n",
    "            X_train, y_train = cls.split_Xy(train, timesteps)\n",
    "            X_test, y_test = cls.split_Xy(test, timesteps)\n",
    "            return X_train, y_train, X_test, y_test\n",
    "\n",
    "        return cls.split_Xy(df, timesteps)\n",
    "\n",
    "    @classmethod\n",
    "    def aggrupate_in_timesteps(cls, data: ndarray, timesteps: int) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Given a dataframe, divide the sequence into multiple input/output \n",
    "        patterns called samples, where a number of time steps are used as \n",
    "        input and one time step is used as output for the one-step prediction \n",
    "        that is being learned.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        series = df.copy(deep=True)\n",
    "        series_s = series.copy(deep=True)\n",
    "        for i in range(timesteps):\n",
    "            series = pd.concat([series, series_s.shift(-(i + 1))], axis=1)\n",
    "        series.dropna(axis=0, inplace=True)\n",
    "        return series\n",
    "\n",
    "    @classmethod\n",
    "    def split_Xy(cls, data: ndarray, timesteps) -> Tuple[ndarray, ndarray]:\n",
    "        \"\"\"\n",
    "        Take num_samples from data, and separate X and y from it into two\n",
    "        new tensors that will be used to feed the LSTM.\n",
    "        \"\"\"\n",
    "        num_samples = data.shape[0]\n",
    "        num_categories = int(data.shape[1] / (timesteps + 1))\n",
    "        subset = np.array(data).reshape((num_samples, timesteps + 1, num_categories))\n",
    "\n",
    "        X = subset[:, 0:timesteps, :]\n",
    "\n",
    "        # maybe, data is encoded. In that case I must keep, NOT the last column\n",
    "        # but the last $ num_categories / 4 $, which corresponds to the encoded\n",
    "        # values of the last column, corresponding to the \"close\" value.\n",
    "        from_column_index = int(num_categories - (num_categories / 4))\n",
    "        to_column_index = num_categories\n",
    "        y = subset[:, -1, from_column_index:to_column_index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    @classmethod\n",
    "    def _last_index_in_training(\n",
    "        cls, df: DataFrame, timesteps: int, test_size: float = 0.1\n",
    "    ) -> int:\n",
    "        \"\"\"Returns the last value in the training set, from the original dataframe\"\"\"\n",
    "        train_size = 1.0 - test_size\n",
    "        n_samples = df.shape[0]\n",
    "        n_test = ceil(test_size * n_samples)\n",
    "        n_train = floor(train_size * n_samples)\n",
    "        last_index_in_train = n_train - timesteps - 1\n",
    "        return last_index_in_train\n",
    "\n",
    "    @classmethod\n",
    "    def last_in_training(\n",
    "        cls, df: DataFrame, timesteps: int, test_size: float = 0.1\n",
    "    ) -> Series:\n",
    "        return ticks.data.iloc[cls._last_index_in_training(df, timesteps, test_size)]\n",
    "\n",
    "    @classmethod\n",
    "    def first_in_test(\n",
    "        cls, df: DataFrame, timesteps: int, test_size: float = 0.1\n",
    "    ) -> Series:\n",
    "        return ticks.data.iloc[\n",
    "            cls._last_index_in_training(df, timesteps, test_size) + 1\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Test Sequences (success !!) ðŸ˜€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the 100 OHLC values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticks shape: (480, 4)\n",
      "[[0.71 0.66 0.61 0.67]\n",
      " [0.73 0.78 0.77 0.79]\n",
      " [0.9  0.99 0.96 1.  ]\n",
      " [1.07 1.13 1.13 1.17]\n",
      " [1.26 1.21 1.26 1.22]]\n"
     ]
    }
   ],
   "source": [
    "timesteps = 4\n",
    "\n",
    "ticks = Ticks(url, scale=True)\n",
    "df = ticks.data.copy(deep=True)\n",
    "print(f\"Ticks shape: {ticks.data.shape}\\n{df.head().values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ticks encoded shape: (480, 60)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ticks_encoded = ticks.encode()\n",
    "print(f\"Ticks encoded shape: {ticks_encoded.shape}\")\n",
    "print(ticks_encoded[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      " [[[0.71 0.66 0.61 0.67]\n",
      "  [0.73 0.78 0.77 0.79]\n",
      "  [0.9  0.99 0.96 1.  ]\n",
      "  [1.07 1.13 1.13 1.17]]\n",
      "\n",
      " [[0.73 0.78 0.77 0.79]\n",
      "  [0.9  0.99 0.96 1.  ]\n",
      "  [1.07 1.13 1.13 1.17]\n",
      "  [1.26 1.21 1.26 1.22]]\n",
      "\n",
      " [[0.9  0.99 0.96 1.  ]\n",
      "  [1.07 1.13 1.13 1.17]\n",
      "  [1.26 1.21 1.26 1.22]\n",
      "  [1.25 1.23 1.29 1.24]]] \n",
      "y_train:\n",
      " [[1.22]\n",
      " [1.24]\n",
      " [1.13]]\n",
      "---\n",
      "X_test:\n",
      " [[[-0.   -0.09 -0.05 -0.03]\n",
      "  [-0.03 -0.03  0.04  0.03]\n",
      "  [ 0.08  0.07  0.15  0.13]\n",
      "  [ 0.13  0.11  0.17  0.19]]\n",
      "\n",
      " [[-0.03 -0.03  0.04  0.03]\n",
      "  [ 0.08  0.07  0.15  0.13]\n",
      "  [ 0.13  0.11  0.17  0.19]\n",
      "  [ 0.24  0.18  0.08  0.  ]]\n",
      "\n",
      " [[ 0.08  0.07  0.15  0.13]\n",
      "  [ 0.13  0.11  0.17  0.19]\n",
      "  [ 0.24  0.18  0.08  0.  ]\n",
      "  [-0.03 -0.13 -0.29 -0.37]]] \n",
      "y_test:\n",
      " [[ 0.  ]\n",
      " [-0.37]\n",
      " [-0.27]]\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = sequences.prepare(\n",
    "    ticks.data, timesteps=timesteps, test_size=0.1\n",
    ")\n",
    "\n",
    "print(\"X_train:\\n\", X_train[0:3], \"\\ny_train:\\n\", y_train[0:3])\n",
    "print(\"---\")\n",
    "print(\"X_test:\\n\", X_test[0:3], \"\\ny_test:\\n\", y_test[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Experimentation here !! ðŸ‘¨â€ ðŸ”¬ ðŸ§ª\n",
    "\n",
    "## Metrics and Results display (âš¡ï¸)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fail(y, y_prev, yhat):\n",
    "    return np.sign(y - y_prev) != np.sign(yhat - y_prev)\n",
    "\n",
    "\n",
    "def plot_and_compare(df: DataFrame):\n",
    "    fails = 0\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    for i in range(1, df.shape[0]):\n",
    "        segment_color = \"red\"\n",
    "        lw = 0.8\n",
    "        alpha = 0.6\n",
    "        if i > 0:\n",
    "            if not fail(df.iloc[i].y, df.iloc[i - 1].y, df.iloc[i].yhat):\n",
    "                segment_color = \"green\"\n",
    "                lw = 1.0\n",
    "                alpha = 1.0\n",
    "            else:\n",
    "                fails += 1\n",
    "\n",
    "        plt.plot(\n",
    "            [i - 1, i],\n",
    "            [df.y.iloc[i - 1], df.y.iloc[i]],\n",
    "            marker=\".\",\n",
    "            linewidth=0.8,\n",
    "            linestyle=\"--\",\n",
    "            alpha=0.4,\n",
    "            color=\"black\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            [i - 1, i],\n",
    "            [df.y.iloc[i - 1], df.yhat.iloc[i]],\n",
    "            linewidth=1.0,\n",
    "            # alpha=alpha,\n",
    "            color=segment_color,\n",
    "        )\n",
    "\n",
    "    hits = df.shape[0] - fails\n",
    "    plt.title(\n",
    "        f\"Aciertos ({hits}/{100*(hits/df.shape[0]):.2f}%) y fallos ({fails}/{100*(fails/df.shape[0]):.2f}%) de tendencia\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def trend_performance(df):\n",
    "    fails = 0\n",
    "    for i in range(1, df.shape[0]):\n",
    "        if i > 0:\n",
    "            if fail(df.iloc[i].y, df.iloc[i - 1].y, df.iloc[i].yhat):\n",
    "                fails += 1\n",
    "    return (df.shape[0] - fails) / df.shape[0]\n",
    "\n",
    "\n",
    "def mean_error(df):\n",
    "    cum_distance = 0.0\n",
    "    for i in range(1, df.shape[0]):\n",
    "        cum_distance += np.abs(df.y.iloc[i] - df.yhat.iloc[i])\n",
    "    return cum_distance / df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data reading (and/or encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (428, 4, 4), y_train(428, 1)\n",
      "X_test (48, 4, 4), y_test(48, 1)\n"
     ]
    }
   ],
   "source": [
    "window_size = 4\n",
    "test_size = 0.1\n",
    "\n",
    "ticks = Ticks(url, scale=True)\n",
    "# ticks.encode()\n",
    "X_train, y_train, X_test, y_test = sequences.prepare(\n",
    "    ticks.data, timesteps=window_size, test_size=test_size\n",
    ")\n",
    "print(f\"X_train {X_train.shape}, y_train{y_train.shape}\")\n",
    "print(f\"X_test {X_test.shape}, y_test{y_test.shape}\")\n",
    "\n",
    "# This is to know if data is coming in encoded (onehot) or not.\n",
    "binary_encoded = ((y_train == 0.0) | (y_train == 1.0)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1units = 128\n",
    "l2units = 128\n",
    "dropout = 0.1\n",
    "activation = \"tanh\"\n",
    "\n",
    "num_categories = X_train.shape[2]\n",
    "num_predictions = y_train.shape[1]\n",
    "tensorboard = True\n",
    "\n",
    "loss = \"binary_crossentropy\" if binary_encoded else \"mean_squared_error\"\n",
    "optimizer = \"adam\"\n",
    "metrics = [\"categorical_crossentropy\"] if binary_encoded else [\"mse\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM creation / TensorBoard launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 4, 128)            68096     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 199,809\n",
      "Trainable params: 199,809\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Launching TensorBoard..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-155093c455e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rm -rf logs/*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load_ext'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorboard'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'--logdir logs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/trader/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2324\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2326\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2327\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/trader/lib/python3.7/site-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36m_start_magic\u001b[0;34m(line)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_start_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;34m\"\"\"Implementation of the `%tensorboard` line magic.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/trader/lib/python3.7/site-packages/tensorboard/notebook.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(args_string)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0mparsed_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshlex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m     \u001b[0mstart_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStartLaunched\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/trader/lib/python3.7/site-packages/tensorboard/manager.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(arguments, timeout)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0mend_time_seconds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_time_seconds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mend_time_seconds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_interval_seconds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0msubprocess_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msubprocess_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(123)\n",
    "model = Sequential()\n",
    "model.add(\n",
    "    LSTM(\n",
    "        input_shape=(window_size, num_categories),\n",
    "        return_sequences=True,\n",
    "        units=l1units,\n",
    "        kernel_regularizer=l2(0.0000001),\n",
    "        activity_regularizer=l2(0.0000001),\n",
    "    )\n",
    ")\n",
    "model.add(Dropout(dropout))\n",
    "model.add(\n",
    "    LSTM(\n",
    "        l2units, \n",
    "        kernel_regularizer=l2(0.0000001), \n",
    "        activity_regularizer=l2(0.0000001))\n",
    ")\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(num_predictions, activation=activation))\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = []\n",
    "log_dir = (\n",
    "    \"logs/fit/\"\n",
    "    + f\"ws{window_size:02d}_\"\n",
    "    + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    ")\n",
    "callbacks.append(tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1))\n",
    "\n",
    "## TensorBoard launch\n",
    "\n",
    "!rm -rf logs/*\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 600\n",
    "verbose = 0\n",
    "batch_size = 1\n",
    "validation_split = 0.1\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=verbose,\n",
    "    validation_split=validation_split,\n",
    "    callbacks=callbacks,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(X_test)\n",
    "print(f\"Predictions (yhat): {yhat.shape}\")\n",
    "\n",
    "n_predictions = int(X_test.shape[0])\n",
    "print(f\"n_predictions: {n_predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding onehot prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if binary_encoded:\n",
    "    last = sequences.last_in_training(ticks.data, window_size, test_size).close\n",
    "    ticks.decode(yhat[0], last, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decode the entire set of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if binary_encoded:\n",
    "    decoded = []\n",
    "    last_index = sequences._last_index_in_training(ticks.data, window_size, test_size)\n",
    "    last_value = ticks.data.iloc[last_index + window_size].close\n",
    "\n",
    "    for i in range(yhat.shape[0]):\n",
    "        this_value = ticks.decode(yhat[i], last_value)\n",
    "        decoded.append(this_value)\n",
    "        last_index += 1\n",
    "        last_value = ticks.data.iloc[last_index].close\n",
    "\n",
    "    print(f\"decoded size: {len(decoded)}\\n{decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if binary_encoded:\n",
    "    last_index = sequences._last_index_in_training(ticks.data, window_size, test_size)\n",
    "    Y = ticks.data.iloc[last_index + 1 + window_size :].close.values.reshape(\n",
    "        n_predictions,\n",
    "    )\n",
    "    Yhat = np.array(decoded).reshape(n_predictions,)\n",
    "else:\n",
    "    Y = y_test.reshape(n_predictions,)\n",
    "    Yhat = yhat.reshape(n_predictions,)\n",
    "\n",
    "results = pd.DataFrame({\"y\": Y, \"yhat\": Yhat,}).round(2)\n",
    "print(\n",
    "    f\"Mean Err.: {mean_error(results):5.3f}, Trend perf.: {trend_performance(results):4.2f}\"\n",
    ")\n",
    "\n",
    "plot_and_compare(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mores resultados con 400 epocs, batch_size = 1, window_size = 4, unencoded (60% aciertos de tendencia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Summary Results\n",
    "\n",
    "Compare UNencoded version (LSTM feed with floats) against ENCODED version (LSTM encoded with binary arrays).\n",
    "\n",
    "    window_size = 4\n",
    "    test_size = 0.1\n",
    "\n",
    "DAX_2018_2019 (floats)\n",
    "\n",
    "| epochs | Mean err. (unencoded) | Trend perf (unencoded)|\n",
    "|--------|-----------------------|-----------------------|\n",
    "| 100    | 0.082                 | 0.54                  |\n",
    "| 200    | 0.073                 | 0.62                  |\n",
    "| 400    | 0.078                 | 0.58                  |\n",
    "| 800    | 0.085                 | 0.58                  |\n",
    "\n",
    "DAX_2018_2019 (onehot)\n",
    "\n",
    "| epochs | Mean err. (encoded) | Trend perf (encoded)|\n",
    "|--------|---------------------|---------------------|\n",
    "| 100    | 0.374               | 0.40                |\n",
    "| 200    | 0.308               | 0.40                |\n",
    "| 400    | 0.298               | 0.46                |\n",
    "| 800    | 0.307               | 0.35                |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = DataFrame(\n",
    "    {\n",
    "        \"mean error unencoded\": [0.082, 0.073, 0.078, 0.085],\n",
    "        \"mean error encoded\": [0.374, 0.308, 0.298, 0.307],\n",
    "        \"trend performance unencoded\": [0.54, 0.62, 0.58, 0.58],\n",
    "        \"trend performance encoded\": [0.40, 0.40, 0.46, 0.35],\n",
    "    },\n",
    "    index=[100, 200, 400, 800],\n",
    ")\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# plt.figure(figsize=(12, 7))\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "fig.set_size_inches(14, 5)\n",
    "res_df[[\"mean error unencoded\", \"mean error encoded\"]].plot(\n",
    "    kind=\"bar\", ax=axes[0], alpha=0.6, ylim=(0.0, 1.0)\n",
    ")\n",
    "\n",
    "axes[1].axhline(0.5, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "res_df[[\"trend performance unencoded\", \"trend performance encoded\"]].plot(\n",
    "    kind=\"bar\", ax=axes[1], alpha=0.6, ylim=(0.0, 1.0)\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
